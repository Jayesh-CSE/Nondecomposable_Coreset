{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import all packages \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.rand((5,6))\n",
    "#X = np.random.rand(5,6)\n",
    "y = torch.tensor([-1, 1, -1, 1, 1])\n",
    "#y = np.array([-1, 1, -1, 1, 1])\n",
    "w = torch.rand(X.shape[1], requires_grad = True)\n",
    "b = 0.5 #threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified selection of class labels:(array([-1,  1], dtype=int32), array([ 856, 1144])) \n",
      "tensor([ 0.9819, -0.4815,  0.6332,  0.7133,  0.6134, -0.4286,  0.5506,\n",
      "        -0.9163, -0.3983, -0.8378,  0.9037, -0.8973,  0.5829,  0.1902,\n",
      "        -0.6596,  0.2781,  0.4921,  0.8717,  0.3093,  0.8141,  0.5505,\n",
      "         0.3575, -0.8456, -0.7688,  0.9862, -0.0005, -0.0179,  0.6251,\n",
      "        -0.2774, -0.6065,  0.8751,  0.7457,  0.6839,  0.8273, -0.7167,\n",
      "        -0.9226, -0.1978,  0.0525, -0.1350, -0.4951,  0.7814,  0.9631,\n",
      "         0.8441,  0.8857,  0.3829,  0.9864, -0.0120,  0.1877, -0.5585,\n",
      "        -0.7464, -0.2764,  0.3727, -0.7605, -0.4680,  0.2375,  0.7660])\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load(\"F1_2000_x_train.npy\")\n",
    "\n",
    "x_test = np.load(\"F1_2000_x_test.npy\")\n",
    "\n",
    "y_train = np.load(\"F1_2000_y_train.npy\")\n",
    "\n",
    "y_test = np.load(\"F1_2000_y_test.npy\")\n",
    "\n",
    "print(\"Stratified selection of class labels:{} \".format(np.unique(y_train, return_counts=True)))\n",
    "\n",
    "\n",
    "mm = make_pipeline(MinMaxScaler(), Normalizer())\n",
    "x_train = mm.fit_transform(x_train)\n",
    "x_test = mm.transform(x_test)\n",
    "\n",
    "ones_train = np.ones(x_train.shape[0])\n",
    "ones_test = np.ones(x_test.shape[0])\n",
    "\n",
    "ones_train.shape = (x_train.shape[0],1)\n",
    "ones_test.shape = (x_test.shape[0],1)\n",
    "\n",
    "x_train = np.append(x_train, ones_train, axis = 1)\n",
    "x_test = np.append(x_test, ones_test, axis = 1)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_test = torch.from_numpy(x_test)\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "#w = torch.rand(x_train.shape[1], requires_grad = True)\n",
    "\n",
    "w = torch.tensor(torch.FloatTensor(x_train.shape[1]).uniform_(-1, 1), requires_grad = True)\n",
    "\n",
    "print(w)\n",
    "\n",
    "b = 0.5 #threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pos = []\n",
    "data_neg = []\n",
    "\n",
    "for i in range(y_train.shape[0]):    \n",
    "    if(y_train[i] == 1):\n",
    "        data_pos.append(i)\n",
    "    elif(y_train[i] == -1):\n",
    "        data_neg.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pos_len = len(data_pos)\n",
    "y_neg_len = len(data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144\n",
      "856\n"
     ]
    }
   ],
   "source": [
    "print(y_pos_len)\n",
    "print(y_neg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_loss(X, y, w, b):\n",
    "    sum_l = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        #sum_l = sum_l + max(0, 1-y[i].double()*(torch.sign(torch.dot(w,X[i])) - b))\n",
    "        #sum_l = sum_l + max(0, 1-y[i].double()*(torch.dot(w,X[i]).double() - b))\n",
    "        sum_l = sum_l + max(0, 1-y[i]*(torch.dot(w,X[i]) - b))\n",
    "    return sum_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_loss(x_train.float(),y_train.float(),w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epoch = 100\n",
    "\n",
    "loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "\n",
    "langrange_lambda = torch.tensor(0.01, dtype=torch.float, requires_grad=True)\n",
    "psi = torch.tensor([y_pos_len - loss_pos], requires_grad= True).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_epoch):\n",
    "    \n",
    "    print(f\"Inside {i}\")\n",
    "\n",
    "    loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "    loss_neg = sum_loss(x_train[data_neg].float(), y_train[data_neg].float(), w, b)\n",
    "    \n",
    "    #print(f\"Before backword: {w.grad}\")\n",
    "    \n",
    "    #print(f\"Before backword: {psi.grad}\")\n",
    "    \n",
    "    w.retain_grad()\n",
    "    \n",
    "\n",
    "    final_loss = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double() + ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    final_loss.backward(retain_graph=True)\n",
    "    \n",
    "    \n",
    "    w.retain_grad()\n",
    "\n",
    "\n",
    "    #print(f\"After Backword: {w.grad}\")\n",
    "    #print(f\"After Backword: {psi.grad}\")\n",
    "    \n",
    "    w = w - learning_rate * w.grad\n",
    "    \n",
    "    \n",
    "    loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "    loss_neg = sum_loss(x_train[data_neg].float(), y_train[data_neg].float(), w, b)\n",
    "    \n",
    "    \n",
    "    psi.retain_grad()\n",
    "    \n",
    "    final_loss_psi = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double()+ ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    final_loss_psi.backward(retain_graph=True)\n",
    "    \n",
    "    psi.retain_grad()\n",
    "    \n",
    "    psi = psi - learning_rate * psi.grad\n",
    "    \n",
    "    #print(f\"After update: {w.grad}\")\n",
    "    #print(f\"After update: {psi.grad}\")\n",
    "    \n",
    "    \n",
    "    #w.grad.zero_()\n",
    "    #psi.grad.zero_()\n",
    "    #langrange_lambda.grad.zero_()\n",
    "\n",
    "    loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "    loss_neg = sum_loss(x_train[data_neg].float(), y_train[data_neg].float(), w, b)\n",
    "    \n",
    "    \n",
    "    langrange_lambda.retain_grad()\n",
    "\n",
    "    final_loss_a = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double() + ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    final_loss_a.backward(retain_graph=True)\n",
    "    \n",
    "    langrange_lambda.retain_grad()\n",
    "    #(final_loss_a).backward()\n",
    "    langrange_lambda = langrange_lambda + learning_rate * langrange_lambda.grad\n",
    "    \n",
    "    loss_after_update = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double() + ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    \n",
    "    print(f\"Loss value at {i}th epoch is: {loss_after_update}\")\n",
    "\n",
    "    #w.grad.zero_()\n",
    "    #psi.grad.zero_()\n",
    "    #langrange_lambda.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w)\n",
    "print(psi)\n",
    "print(langrange_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w.grad)\n",
    "print(langrange_lambda.grad)\n",
    "print(psi.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_epoch):\n",
    "    \n",
    "    print(f\"Inside {i}\")\n",
    "    \n",
    "    \n",
    "    loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "    loss_neg = sum_loss(x_train[data_neg].float(), y_train[data_neg].float(), w, b)\n",
    "    \n",
    "    \n",
    "    langrange_lambda.retain_grad()\n",
    "\n",
    "    final_loss_a = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double() + ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    final_loss_a.backward(retain_graph=True)\n",
    "    \n",
    "    langrange_lambda.retain_grad()\n",
    "    #(final_loss_a).backward()\n",
    "    langrange_lambda = langrange_lambda + learning_rate * langrange_lambda.grad\n",
    "    \n",
    "\n",
    "    loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "    loss_neg = sum_loss(x_train[data_neg].float(), y_train[data_neg].float(), w, b)\n",
    "    \n",
    "    #print(f\"Before backword: {w.grad}\")\n",
    "    \n",
    "    #print(f\"Before backword: {psi.grad}\")\n",
    "    \n",
    "    w.retain_grad()\n",
    "    \n",
    "\n",
    "    final_loss = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double() + ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    final_loss.backward(retain_graph=True)\n",
    "    \n",
    "    \n",
    "    w.retain_grad()\n",
    "\n",
    "\n",
    "    #print(f\"After Backword: {w.grad}\")\n",
    "    #print(f\"After Backword: {psi.grad}\")\n",
    "    \n",
    "    w = w - learning_rate * w.grad\n",
    "    \n",
    "    \n",
    "    loss_pos = sum_loss(x_train[data_pos].float(), y_train[data_pos].float(), w, b)\n",
    "    loss_neg = sum_loss(x_train[data_neg].float(), y_train[data_neg].float(), w, b)\n",
    "    \n",
    "    \n",
    "    psi.retain_grad()\n",
    "    \n",
    "    final_loss_psi = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double()+ ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    final_loss_psi.backward(retain_graph=True)\n",
    "    \n",
    "    psi.retain_grad()\n",
    "    \n",
    "    psi = psi - learning_rate * psi.grad\n",
    "    \n",
    "    #print(f\"After update: {w.grad}\")\n",
    "    #print(f\"After update: {psi.grad}\")\n",
    "    \n",
    "    \n",
    "    #w.grad.zero_()\n",
    "    #psi.grad.zero_()\n",
    "    #langrange_lambda.grad.zero_()\n",
    "    \n",
    "    loss_after_update = (1.0/psi) * torch.tensor(loss_neg).double() + langrange_lambda.double() * torch.tensor(loss_pos).double() + ((1.0/psi) - langrange_lambda.double()) * y_pos_len + psi * langrange_lambda.double()\n",
    "    \n",
    "    print(f\"Loss value at {i}th epoch is: {loss_after_update}\")\n",
    "\n",
    "    #w.grad.zero_()\n",
    "    #psi.grad.zero_()\n",
    "    #langrange_lambda.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w)\n",
    "print(psi)\n",
    "print(langrange_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score_pr(precision, recall):\n",
    "        \n",
    "    denom = precision+ recall\n",
    "\n",
    "    if(denom == 0):\n",
    "        denom = 1\n",
    "    f1 = 2 * (precision * recall) / (denom)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = np.dot(x_test, w.detach().numpy())\n",
    "y_hat[y_hat > 0] = 1\n",
    "y_hat[y_hat < 0] = -1\n",
    "print(\"Prediction vector y_hat: {}\".format(np.unique(y_hat, return_counts= True)))\n",
    "precision = metrics.precision_score(y_test, y_hat)\n",
    "recall = metrics.recall_score(y_test, y_hat)\n",
    "f1 = f1_score_pr(precision, recall)\n",
    "print(\"f1 score : {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
